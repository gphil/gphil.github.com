<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <title>gphil's blog - All posts</title>
    <link href="http://gphil.net/rss.xml" rel="self" />
    <link href="http://gphil.net" />
    <id>http://gphil.net/rss.xml</id>
    <author>
        <name>Greg Phillips</name>
        <email>gap023@gmail.com</email>
    </author>
    <updated>2014-01-21T00:00:00Z</updated>
    <entry>
    <title>A Really Simple Product Development Workflow</title>
    <link href="http://gphil.net/posts/2014-01-21-a-really-simple-product-development-workflow.html" />
    <id>http://gphil.net/posts/2014-01-21-a-really-simple-product-development-workflow.html</id>
    <published>2014-01-21T00:00:00Z</published>
    <updated>2014-01-21T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>As we’re looking bring on some more development talent at <a href="https://kwelia.com">Kwelia</a>, I’ve been reflecting a lot on what processes have worked for us as a founding team so far in terms of product development. I wanted to start with something really simple. In my experience, having too much process can feel overly restrictive and often gets circumvented. Having too little process results in a lack of direction (which can be fatal for a startup.)</p>
<p>So, I came up with the following (click to enlarge):</p>
<p><a href="/img/pf.png" data-lightbox="A Really Simple Product Development Workflow" title="Yahoo Homepage 2012-07-15"><img src="/img/pf.png" alt="A Really Simple Product Development Workflow" width="500"></a></p>
<p>It’s obviously oversimplified, but I think it codifies fairly well what’s worked for us in terms of product development. And, upon reflection, most of the effort that’s fallen outside of this framework hasn’t been very fruitful. I’m going to try cylcing through this workflow for a while and see if I’m missing anything important.</p>]]></summary>
</entry>
<entry>
    <title>Yahoo's New Direction</title>
    <link href="http://gphil.net/posts/2013-09-08-yahoos-new-direction.html" />
    <id>http://gphil.net/posts/2013-09-08-yahoos-new-direction.html</id>
    <published>2013-09-08T00:00:00Z</published>
    <updated>2013-09-08T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Along with a good portion of the rest of the internet, I read <a href="http://marissamayr.tumblr.com/post/60336044815/geeking-out-on-the-logo">Marissa Mayer’s post about the new Yahoo logo</a>. There’s been a lot of talk about the logo since then, and for what it’s worth: I like the new logo more than the old one, and I think that it is notable that they decided to buck the “flat” design trend by giving it some depth.</p>
<p>However, what Mayer’s post really got me thinking about was: how has Yahoo changed since she took over?</p>
<p>Obviously, in addition to the new logo, they’re getting a lot more attention from the tech media, and they’ve made a <a href="http://en.wikipedia.org/wiki/List_of_mergers_and_acquisitions_by_Yahoo!#Acquisitions">bunch of acquistions</a>.</p>
<p>But how has all this actually affected their web presence? I decided to take a look at the Yahoo homepage this evening, and compare it to the Yahoo homepage on the eve of the day Mayer was named CEO. Take a look:</p>
<p>Yahoo Homepage as of July 15th 2012:</p>
<p><a href="/img/yahoo-homepage-2012-07-15.png" data-lightbox="image-1" title="Yahoo Homepage 2012-07-15"><img src="/img/yahoo-homepage-2012-07-15.png" alt="Yahoo Homepage 2012-07-15" width="500"></a></p>
<p>Yahoo Homepage as of September 8th 2013:</p>
<p><a href="/img/yahoo-homepage-2013-09-08.png" data-lightbox="image-2" title="Yahoo Homepage 2013-09-08"><img src="/img/yahoo-homepage-2013-09-08.png" alt="Yahoo Homepage 2013-09-08" data-lightbox="image-2" title="Yahoo Homepage 2013-09-08" width="500"></a></p>
<p>Although the pages have very different visual feels, there has been little substantive change: the layout and content are nearly identical.</p>
<p>The biggest difference is the introduction of the navigation bar similar to Google’s at the top of today’s page. This seems to represent a shift in focus from being a content provider to more of an application provider like Google is.</p>
<p>In the last year or so, Yahoo has redesigned Flickr and Mail and acquired a bunch of other “apps” (the most prominent of which being Tumblr.) However, none of the acquisitions seem to have made a dent in the homepage yet.</p>
<p>So, even though Yahoo has a new logo to go with its new look and feel, it still seems to be more or less the same site–even after all the acquisitions. This might change, but for now it seems that the changes at Yahoo are mostly skin-deep.</p>]]></summary>
</entry>
<entry>
    <title>My Quantified Self Setup</title>
    <link href="http://gphil.net/posts/2013-08-31-my-quantified-self-setup.html" />
    <id>http://gphil.net/posts/2013-08-31-my-quantified-self-setup.html</id>
    <published>2013-08-31T00:00:00Z</published>
    <updated>2013-08-31T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>A few years ago, I didn’t keep track of any personal metrics, nor did I feel there was any need to. This has changed drastically since then: I now keep track of metrics pertaining to my work habits, career, exercise, diet and sleep. As others have observed before, I found that merely keeping track of these metrics has positively affected my focus, behavior and overall health. However, it didn’t happen overnight–it took a progression of deliberate incremental steps over time.</p>
<p>I started with <a href="https://www.rescuetime.com/">RescueTime</a> shortly after co-founding my startup, <a href="https://www.kwelia.com/">Kwelia</a>. Kwelia is my first venture, and I left a more structured software engineering job at an established company to start it. My initial motivation was that I didn’t want my work ethic to slip when my job became more self-directed. This didn’t happen for a number of reasons, but I think RescueTime’s tracking of all of my behavior on my computer certainly helped me stay focused. (I could really use RescueTime or a RescueTime-like for my iPhone as well–it seems like that’s where I waste the most time now, but I digress.)</p>
<p>After the initial success with RescueTime, I was thinking about how I could measure progress on more of a career and company development level. I came across this <a href="http://calnewport.com/blog/2013/04/08/deliberately-experimenting-with-deliberate-practice-looking-for-subjects-to-test-my-advice/">blog post</a> by <a href="http://calnewport.com">Cal Newport</a> announcing a class that would explore the notion of <a href="https://en.wikipedia.org/wiki/Practice_learning_method#Deliberate_practice">deliberate practice</a> in a career-development context, and my interest was piqued. I ended up taking the class, and the biggest take-away for me was the isolation of some key career metrics that I wanted to track. I won’t go into too much detail, but they all boiled to down to gaining more traction for <a href="https://kwelia.com/">Kwelia</a>, as well as for my personal brand via writing and tweeting more. I now periodically track these metrics manually in a small Emacs <a href="https://org-mode.org">org-mode</a> document. Just keeping track of a few rough measures of “traction” has inspired me to work harder to promote myself and my company.</p>
<p>The final piece of my current Quantified Self puzzle fell into place when my <a href="https://twitter.com/jessdabel">lovely, fitness-obsessed girlfriend</a> bought me a <a href="http://www.fitbit.com/">FitBit</a>. Initially, I thought I would just wear it and track my steps as a rough gauge of how much physical activity I was getting. However, I ended up going whole-hog and tracking my sleep, calories consumed and other fitness activity using their software as well. I’ve been doing this for almost a month now, and I’ve been losing weight and finding it easier to make healthier choices when it comes to my diet and exercise just by keeping track.</p>
<p>If there’s one lesson I’ve learned from all of this, it’s that you can initiate a tremendous amount of personal change just by starting to keep track of metrics in areas where you wish to improve. If you dutifully keep track of your past activity, you will naturally be more mindful of your goals when making decisions in the future. I underestimated the power of this phenomenon going into it, but now I’m slowly but surely becoming a quantified-self addict.</p>]]></summary>
</entry>
<entry>
    <title>Don't Get Lost in Big Data</title>
    <link href="http://gphil.net/posts/2013-08-01-dont-get-lost-in-big-data.html" />
    <id>http://gphil.net/posts/2013-08-01-dont-get-lost-in-big-data.html</id>
    <published>2013-08-01T00:00:00Z</published>
    <updated>2013-08-01T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>The value of data comes only when one can derive actionable insights from it. Accordingly, the primary aim of a big data project should be to convert big data into useful data.</p>
<p>However, it seems like a lot of big data projects lose sight of the this goal. I think the reason for this is that these projects quickly run into engineering challenges.</p>
<p>It’s easy to lose the forest for the trees when the focus shifts from the end goal (delivering value to stakeholders) to acheiving difficult intermediate goals (how are we going to efficiently process this data?)</p>
<p>I had this realization after I recently re-read <a href="http://www.crunchbase.com/person/dj-patil">DJ Patil’s</a> essay <a href="http://oreilly.com/data/radarreports/data-jujitsu.csp">Data Jujitsu</a>. The essay is so-named because it talks about how to work around the big engineering challenges that tend to crop up when working with large datasets, without meeting them head-on. I found that it really resonated with some of the findings I’ve had while working on <a href="https://kwelia.com">Kwelia</a>.</p>
<p>These types of “Data Jujitsu” strategies are critical for getting a big data project off the ground–not only because they help you avoid doing extra work, but also because they make it easier to stay focused on the real goal of delivering as much value as possible to the stakeholders of the project.</p>]]></summary>
</entry>
<entry>
    <title>Leiningen Download Stats</title>
    <link href="http://gphil.net/posts/2013-07-11-2013-leiningen-download-stats.html" />
    <id>http://gphil.net/posts/2013-07-11-2013-leiningen-download-stats.html</id>
    <published>2013-07-11T00:00:00Z</published>
    <updated>2013-07-11T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>About a month ago, I wrote a post <a href="2013-06-10-tracking-clojure-versus-ruby-adoption.html">comparing Ruby and Clojure adoption</a> based on package respository download stats and some other metrics.</p>
<p>At the time, statistics on the number of downloads for <a href="http://http://leiningen.org/">Leiningen</a> were not available. However, after the blog post was published, <a href="http://technomancy.us/">Phil Hagelberg</a> (the creator of Leiningen) reached out to me letting me know that the stats could be made available if someone were to write some log parsing code to extract them.</p>
<p>So, I recently wrote a little <a href="https://github.com/technomancy/leiningen/pull/1252">log parsing code</a>. Here are the results as of today:</p>
<table class="leiningen-downloads">
        <tr>
               <td>
Downloads from GitHub (through 1/21/2013)
</td>
               <td>
958,033
</td>
        </tr>
        <tr>
               <td>
Downloads from S3 (1/21/2013 until today)
</td>
               <td>
467,730
</td>
        </tr>
        <tr>
               <td>
Total
</td>
               <td>
1,425,763
</td>
        </tr>
        <tr>
                <td>
<br />
</td>
                <td>
<br />
</td>
        </tr>
        <tr>
                <td>
Unique IP Addresses (S3 Downloads Only)
</td>
                <td>
35,888
</td>
        </tr>

</table>

<p>The discrepancy between the unique downloads and total seems to be due to the fact that certain versions of leiningen have been downloaded much more than average. It seems as if a bot or deployment script gone wild has downloaded the same version over and over.</p>
<p>In fact, versions 2.0.0-preview10 and and 2.1.2 account for over a million of the downloads! For comparison, the next most popular version (1.7.1) has only been downloaded 82,444 times. So there’s a little bit of a mystery there.</p>
<p>Let me know if you have any theories!</p>]]></summary>
</entry>
<entry>
    <title>Tracking Clojure vs. Ruby Adoption</title>
    <link href="http://gphil.net/posts/2013-06-10-tracking-clojure-versus-ruby-adoption.html" />
    <id>http://gphil.net/posts/2013-06-10-tracking-clojure-versus-ruby-adoption.html</id>
    <published>2013-06-10T00:00:00Z</published>
    <updated>2013-06-10T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>If you read my <a href="2013-06-07-thoughts-on-clojure-versus-ruby-for-startups.html">previous post</a>, then you know that I’m rooting for greater adoption of Clojure.</p>
<p>In this post I want to take a more quantitative look at the adoption of Clojure, and benchmark that against Ruby. I picked Ruby because it’s another dynamic language that I use frequently, and because I feel that it has quite a bit of traction.</p>
<p>What follows below is an un-scientific comparison of Clojure and Ruby based on usage statistics culled from their respective package repositories, StackOverflow and GitHub.</p>
<h2 id="package-repositories">Package Repositories</h2>
<p>The package repository for Clojure is <a href="https://clojars.org">Clojars</a>. Since its inception, it’s had 4,215,857 total packages downloaded.</p>
<p>(Edit: <a href="https://twitter.com/xeqixeqi">@xeqixeqi</a> pointed out that Clojars started keeping track of downloads in Oct./Nov. 2012, so these numbers don’t actually go back that far. That may explain why they are lagging Ruby by the most. RubyGems has been keeping track since July 2009.)</p>
<p>The corresponding package repository for Ruby is <a href="https://rubygems.org">RubyGems</a>. RubyGems has 1,677,981,312 total downloads. This is ~398x more than Clojars.</p>
<p>The top package on Clojars is <a href="https://github.com/ring-clojure/ring">Ring</a>, which has been downloaded 131,383 times.</p>
<p>The top package on RubyGems is <a href="https://github.com/wycats/thor">Thor</a> and it has been downloaded 30,235,277 times, which is ~230x more than Ring.</p>
<p>Thor is a tool for making command line utilities and is often also used as a build tool, while Ring is a web application library. The de facto build tool for Clojure is <a href="https://github.com/technomancy/leiningen">Leiningen</a>, which might compare more favorably to Thor. However, it is generally downloaded directly and then used to fetch dependencies from Clojars. So far as I know, no statistics are available on the number of Leiningen downloads.</p>
<p>A more apples-to-apples comparison might be to compare Ring to <a href="https://github.com/rack/rack">Rack</a>, its Ruby analogue:</p>
<p>Rack has 27,119,992 downloads which is ~206x the number of Ring downloads.</p>
<h2 id="stackoverflow">StackOverflow</h2>
<p>I used <a href="http://data.stackexchange.com/stackoverflow/query/90306/compare-size-and-growth-trends-for-stackoverflow-tags">this StackExchange Data Explorer query</a> to compare the number of questions tagged as “clojure” or “ruby” on StackOverflow.</p>
<p>The tag “clojure” has been used 1,919 times in the last 6 months, whereas the tag “ruby” has been used 23,495 times in the last 6 months.</p>
<p>This is a much more favorable comparision for Clojure, as Ruby “only” has a ~12x advantage here.</p>
<h2 id="github">GitHub</h2>
<p>Ring has 1,321 stars, and 166 forks and Rack has 1,973 stars and 596 forks for ratios of ~3.5x and ~1.5x respectively.</p>
<p>Looking at <a href="https://github.com/ring-clojure/ring">Ring</a> and <a href="https://github.com/rack/rack">Rack</a> projects on GitHub, the numbers bode even better for Clojure.</p>
<p>Interestingly, it looks like Ring and Rack both started out on personal accounts and subsequently moved to their own organizations. The number of stars are summations using both accounts, but the number of forks reported was the same on both accounts because both organizations have forked from the original personal projects.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Ruby definitely still has a major adoption and mind-share advantage over Clojure, but Clojure fared better than I thought it would on StackOverflow, and especially on GitHub.</p>
<p>It’s also interesting to note the wide range across the comparisons (400x down to 1.5x), which probably says as much about the communities they came from (as well as the inaccuracy of this methodology) as it does for Clojure versus Ruby adoption in the real world.</p>
<p>I look forward to following these numbers in order to see if Clojure makes inroads against Ruby and lowers these ratios over time.</p>]]></summary>
</entry>
<entry>
    <title>Thoughts on Clojure vs. Ruby for Startups</title>
    <link href="http://gphil.net/posts/2013-06-07-thoughts-on-clojure-versus-ruby-for-startups.html" />
    <id>http://gphil.net/posts/2013-06-07-thoughts-on-clojure-versus-ruby-for-startups.html</id>
    <published>2013-06-07T00:00:00Z</published>
    <updated>2013-06-07T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>At <a href="https://kwelia.com">Kwelia</a> we’re building our platform using both Ruby and Clojure. As of today, our web application is a Rails app but we do a lot of backend data processing in Clojure. I personally prefer programming in Clojure to programming in Ruby, and some days I wish we had more Clojure than Ruby in our codebase. I don’t think we’ll be switching from Rails any time soon, but we may try to encapsulate more business logic behind Clojure web services in the future.</p>
<p>When we started building our product (and I still think this is true today–although it’s less true now) it was faster and easier to get a production-ready web application up using Ruby on Rails. One of the best features of Rails in my opinion is that it makes a lot of design decisions for you and that it has pretty sane defaults for the most part. This ends up saving a lot of time up front, which is critical for a start up that wants to quickly validate an idea. There is also a lot of open source code available via RubyGems that replaces or expedites a lot of common web application development tasks. This is a one-two punch that’s pretty hard to beat in a start up context.</p>
<p>However, as the Clojure ecosystem matures it’s starting to chip away at the advantages that the Rails ecosystem offers. More and more quality libraries are being shipped to <a href="https://clojars.org">Clojars</a>, and informal conventions are starting to emerge regarding how to structure a web application in Clojure. I think the growth of both of these are critical for newcomers who want to get started on a Clojure webapp quickly. I’m optimistic that these trends will continue and Clojure will continue to become a better and better choice for new projects.</p>]]></summary>
</entry>
<entry>
    <title>Is It Possible to Securely Meter a Client-Facing API?</title>
    <link href="http://gphil.net/posts/2013-05-12-is-it-possible-to-securely-meter-a-javascript-api.html" />
    <id>http://gphil.net/posts/2013-05-12-is-it-possible-to-securely-meter-a-javascript-api.html</id>
    <published>2013-05-12T00:00:00Z</published>
    <updated>2013-05-12T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Imagine that your business provides a paid API (or since you’re reading this blog, there’s a good chance it actually does), and you want to meter the number of calls to it so that you can bill the API consumers based on their usage.</p>
<p>The best way to do this is to provide a unique secret token to each consumer of the API. Then, you require that the consumer includes the secret token with each API call in order to authenticate that they are allowed to access the API. You can also measure the usage associated with each token. So long as these authentication tokens stay secret, so good.</p>
<p>But what about environments where the API consumer can’t keep their token secret? For example, consider a client-side JavaScript app or a native mobile app where the binary is provided to an untrusted tablet or phone. In both of these cases it is possible for a malicious third party to discover the secret token and authenticate to the API on the legitimate consumer’s behalf (by posing as the consumer using their token.) This is a pretty big issue. There’s basically no way around this if they API needs to be called from an untrusted source. (For more information on this, see <a href="http://www.schneier.com/essay-063.html">this essay</a>.)</p>
<p>It seems to me that the best you can do is to also log the IP address and the top-level domain that the request is originating from (if it’s available.) I think this must be what the <a href="https://developers.google.com/maps/documentation/javascript/reference">Google Maps JavaScript API</a> is doing, as they provide a metered service to client-side JavaScript apps without requiring a secret token. However, these identifiers can also be easily forged or changed by malicious consumers when limits are reached.</p>
<p>You could go a step further and look at the actual usage patterns of the API and decide which are legitimate and which are not based on the legitimate client’s historical usage of the API. But this would be a lot of effort, and would likely be prone to false positives.</p>
<p>I’m not a security expert (just a developer who strives to make secure software) so I don’t know if there are more or better techniques, but it seems like any API accessible from an environment that can be accessed by malicious third parties is susceptible.</p>
<p>It would be interesting to study how the aforementioned Google Maps API is enforcing its metering. Perhaps Google is just willing to accept some amount of unauthorized access? I would love to hear how existing APIs in this category cope with this problem.</p>]]></summary>
</entry>
<entry>
    <title>50 Percent Failure Rate</title>
    <link href="http://gphil.net/posts/2013-04-27-50-percent-failure-rate.html" />
    <id>http://gphil.net/posts/2013-04-27-50-percent-failure-rate.html</id>
    <published>2013-04-27T00:00:00Z</published>
    <updated>2013-04-27T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Lately I’ve been reading <a href="http://www.amazon.com/gp/product/1935401009/ref=as_li_qf_sp_asin_tl?ie=UTF8&amp;camp=1789&amp;creative=9325&amp;creativeASIN=1935401009&amp;linkCode=as2&amp;tag=gphilsblog-20">The Principles of Product Development Flow</a> by Don Reinertsen. It describes a set of principles for conducting product development in an economically optimal fashion.</p>
<p>The principles in the book are aimed at product development “in the large.” The goal of the book is to empower product developers to make better decisions by quantifying the expected economic value of each decision faced when developing a product.</p>
<p>In my case, I’m developing a software product in a startup context (at <a href="https://kwelia.com">Kwelia</a>) and I’ve found that many of principles described in the book have been helpful in guiding our product development.</p>
<p>One of the principles stood out to me as particularly enlightening for a startup: when running an experiment with two outcomes, a 50% failure rate is optimal for generating information.</p>
<p>That’s because if something is certain to succeed or fail, there’s no reason to test it because you already know. If something is likely to fail, you’ll generate a lot of information if it doesn’t actually fail. But in the majority of instances it will indeed fail, and as such this experiment yields little information.</p>
<p>So, the optimal experiment for generating new information has a 50% failure rate. Programmers might recognize this principle from the binary search algorithm.</p>
<p>This principle underlies a lot of what we should be doing when de-risking startups. Binary questions such as “should we do this or not?” or “will that work?” are constantly arising as we generate new ideas.</p>
<p>Applying this principle, we should try to devise experiments that are much more likely to fail (50%) than we might intuitively think, if we want them to generate as much new information as possible.</p>]]></summary>
</entry>
<entry>
    <title>Loading US Census ACS Data into PostgreSQL</title>
    <link href="http://gphil.net/posts/2013-03-05-loading-us-census-acs-data-into-postgresql.html" />
    <id>http://gphil.net/posts/2013-03-05-loading-us-census-acs-data-into-postgresql.html</id>
    <published>2013-03-05T00:00:00Z</published>
    <updated>2013-03-05T00:00:00Z</updated>
    <summary type="html"><![CDATA[<p>Today I was looking into displaying data from the <a href="https://www.census.gov/acs/www/">US Census ACS</a> (the most recent being the 2011 5-year estimates) alongside our analytics at <a href="http://kwelia.com">Kwelia</a>. Unfortunately, this data is only available in a pretty strange format (spread across thousands of text files) that is not easily queried. So, I decided that the first order of business was to get the data loaded into a PostgreSQL database and work from there.</p>
<p>I googled around and found a few projects on GitHub that were built for the purpose of loading this data into PostgreSQL. I settled on <a href="https://github.com/leehach/census-postgres">this one</a>, which did most of what I needed.</p>
<p>However, I had to make a few changes because I only wanted to load the data for the census tract level and ignore the other geographies because we can aggregate everything up from the census tract level in our app. You can check out what I ended up with in <a href="https://github.com/gphil/census-postgres">my version</a>.</p>
<p>It took me a little while to figure this out on my own, so hopefuly this is helpful for other people trying to use this data. Even after loading the data it’s still fairly difficult to figure out what’s where, but at least it’s now in a format that’s easier to manipulate and query.</p>]]></summary>
</entry>

</feed>
